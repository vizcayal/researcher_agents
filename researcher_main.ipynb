{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "%pip install -Uq \"smolagents[mcp,litellm,openai]\" huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8385ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLANNER_PROMPT= \"\"\"\n",
    "You will be given a research task by a user. Your job is to produce a set of\n",
    "instructions for a researcher that will complete the task. Do NOT complete the\n",
    "task yourself, just provide instructions on how to complete it.\n",
    "\n",
    "GUIDELINES:\n",
    "1. Maximize specificity and detail. Include all known user preferences and\n",
    "   explicitly list key attributes or dimensions to consider.\n",
    "2. If essential attributes are missing, explicitly state that they are open-ended.\n",
    "3. Avoid unwarranted assumptions. Treat unspecified dimensions as flexible.\n",
    "4. Use the first person (from the user's perspective).\n",
    "5. When helpful, explicitly ask the researcher to include tables.\n",
    "6. Include the expected output format (e.g. structured report with headers).\n",
    "7. Preserve the input language unless the user explicitly asks otherwise.\n",
    "8. Sources: prefer primary / official / original sources.\n",
    "\"\"\"\n",
    "\n",
    "from hugginface_hub import InferenceClient\n",
    "MODEL_NAME = 'moonshotai/Kimi-K2-Thinking'\n",
    "PROVIDER = 'auto'\n",
    "\n",
    "\n",
    "os.system(\"powershell -c [System.Console]::Beep(440, 500)\")\n",
    "\n",
    "def research_planner(topic:str, model = None) -> str:\n",
    "    logger.Info(f'Starting Planning {topic}')\n",
    "    planner = InferenceClient(\n",
    "        hf_api = os.environ['HF_KEY'],\n",
    "        provider = provider\n",
    "    )\n",
    "    response = planner_client.chat.completion.create(model = model,\n",
    "                                                    message = [\n",
    "                                                        {\"role\":\"system\",\"content\":PLANNER_PROMPT},\n",
    "                                                        {\"role\":\"user\", \"content\":topic}\n",
    "                                                    ])\n",
    "    research_plan = response.choices[0].message.content\n",
    "\n",
    "    logger.Info(\"Generated research\")\n",
    "    logger.Info(f\"{research_plan}\")\n",
    "    return research_plan\n",
    "\n",
    "research_planner(topic=\"use of energy by AI\", model = MODEL_NAME)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd1051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}